{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table align=\"left\">\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/AbdelMahm/INPT-2020/blob/master/Logistic_Regression.ipynb\"><img src=\"https://colab.research.google.com/img/colab_favicon_256px.png\" />Run in Google Colab</a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import urllib.request\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sklearn\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part1: Logistic Regression\n",
    "In this part of the exercise, you will build a logistic regression model to predict whether a student gets admitted into a university.\n",
    "Suppose that you are the administrator of a university department and you want to determine each applicant's chance of admission based on their results on two exams. You have historical data from previous applicants that you can use as a training set for logistic regression. For each training example, you have the applicant's scores on two exams and the admissions decision.\n",
    "Your task is to build a classification model that estimates an applicant's probability of admission based the scores from those two exams."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the data\n",
    "Before starting to implement any learning algorithm, it is always good to visualize the data if possible. In the first part, the code will load the data and display it on a 2-dimensional plot where the axes are the two exam scores, and the positive and negative examples are shown with different marker colors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "data_path = os.path.join(\"datasets\", \"\")\n",
    "download_path = \"https://raw.githubusercontent.com/AbdelMahm/INPT-2020/master/\"\n",
    "os.makedirs(data_path, exist_ok=True)\n",
    "for filename in (\"log_reg_data1.csv\", \"log_reg_data2.csv\"):\n",
    "    print(\"Downloading\", filename)\n",
    "    url = download_path + \"datasets/\" + filename\n",
    "    urllib.request.urlretrieve(url, data_path + filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data\n",
    "data_exam = pd.read_csv(data_path + '/log_reg_data1.csv')\n",
    "data_exam.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the parameters of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.c_[data_exam[[\"score1\",\"score2\"]]]\n",
    "y = np.c_[data_exam[\"admitted\"]]\n",
    "\n",
    "(m,n) = X.shape\n",
    "\n",
    "# display all examples\n",
    "fig = plt.figure()\n",
    "plt.title('Student scores')\n",
    "plt.xlabel('score 1')\n",
    "plt.ylabel('score 2')\n",
    "plt.scatter(X[:,0],X[:,1], c=y.ravel())\n",
    "plt.show()\n",
    "\n",
    "#add a column of 1s to X\n",
    "#X = np.insert(X, 0, values=1, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$w_j$ = clf.coef_, $w_0$ = clf.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression(random_state=0, solver='lbfgs', multi_class='multinomial').fit(X, y.ravel())\n",
    "\n",
    "#print model parameters\n",
    "print(\"w0 =\", clf.intercept_[0], \", w1 = \", clf.coef_[0][0], \", w2 = \", clf.coef_[0][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the decision boundary\n",
    "The decision boundary correspends to the value $y = 0.5$. We can write $x_2$ in terms of $x_1$ by solving the following equation:\n",
    "$$ 0.5 = w_0 + w_1*x_1 + w_2*x_2 $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = plt.axes()\n",
    "plt.title('Students Classification')\n",
    "plt.xlabel('score 1')\n",
    "plt.ylabel('score 2')\n",
    "plt.scatter(X[:,0], X[:,1], c=y.ravel())\n",
    "\n",
    "#generate new points to plot a decision boundary line\n",
    "x1_vals = np.linspace(min(X[:,1]), max(X[:,1]), 1000)\n",
    "# the boundry is at line at y = 0.5, we can then write x2 in terms of x1 (0.5 = theta0 + theta1*x1 + theta2*x2)\n",
    "x2_vals = -(clf.intercept_[0] - 0.5 + clf.coef_[0][0]*x1_vals) / clf.coef_[0][1]\n",
    "\n",
    "# plot the line\n",
    "plt.plot(x1_vals, x2_vals)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy of the model\n",
    "the score function measures how well the learned model predicts on a given set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prediction probability of one example (the 5th example)\n",
    "clf.predict_proba(X[5:6,:]) # the two probabilities sums up to 1.\n",
    "\n",
    "#predicted class of an example (class with max probability)\n",
    "clf.predict(X[5:6,:])\n",
    "\n",
    "#prediction accuracy on the training set X\n",
    "clf.score(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Regularized logistic regression\n",
    "\n",
    "In this part of the exercise, you will implement regularized logistic regression using the ridge method to predict whether microchips from a fabrication plant passes quality assurance (QA). During QA, each microchip goes through various tests to ensure it is functioning correctly.\n",
    "Suppose you are the product manager of the factory and you have the test results for some microchips on two different tests. From these two tests, you would like to determine whether the microchips should be accepted or rejected. To help you make the decision, you have a dataset of test results on past microchips, from which you can build a logistic regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and Visualize the data\n",
    "Similarly to the previous part, we will load and plot the data of the two QA test scores. The positive (y = 1, accepted) and negative (y = 0, rejected) examples are shown with different markers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_microchip = pd.read_csv('datasets/log_reg_data2.csv')\n",
    "data_microchip.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.c_[data_microchip[[\"test1\",\"test2\"]]]\n",
    "y = np.c_[data_microchip[\"accepted\"]]\n",
    "\n",
    "(m,n) = X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1 = X[:,0]\n",
    "X2 = X[:,1]\n",
    "\n",
    "# display\n",
    "fig = plt.figure()\n",
    "plt.title('Microchips tests')\n",
    "plt.xlabel('test 1')\n",
    "plt.ylabel('test 2')\n",
    "plt.scatter(X1,X2, c=y.ravel())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature mapping\n",
    "The scatter plot shows that our dataset cannot be separated into positive and negative examples by a straight-line through the plot. Therefore, a straightforward application of logistic regression will not perform well on this dataset since logistic regression will only be able to find a linear decision boundary.\n",
    "\n",
    "One way to fit the data better is to create more features from each data point. Sklearn provide you with such transformation. PolynomialFeatures allow you to map the features into all polynomial terms of $x_1$ and $x_2$ up to the order power $order$:\n",
    "$$(1, x_1, x_2, x_1^2, x_2^2, x_1x_2, x_1^3, x_1^2x_2, x_2^2x_1, x_2^3, ..., x_2^{order})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "order = 30\n",
    "\n",
    "poly = PolynomialFeatures(order)\n",
    "Xmap = poly.fit_transform(X)\n",
    "\n",
    "print(X.shape)\n",
    "print(Xmap.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a result of a six order power mapping (order=6), our vector of two features (the scores on two QA tests) has been transformed into a 28-dimensional vector. A logistic regression classifier trained on this higher-dimension feature vector will have a more complex decision boundary and will appear nonlinear when drawn in our 2-dimensional plot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fit a logistic regression model to the polynomial features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression(random_state=0, solver='lbfgs', multi_class='multinomial', C=10**7).fit(Xmap, y.ravel())\n",
    "w_star = clf.coef_[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the decision boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_boundary(u, v, theta, order):\n",
    "    boundary = np.zeros(shape=(len(u), len(v)))\n",
    "    for i in range(len(u)):\n",
    "        for j in range(len(v)):\n",
    "            \n",
    "            poly = PolynomialFeatures(order)\n",
    "            uv = [np.array([u[i],v[j]])]\n",
    "            poly_map = poly.fit_transform([np.array([u[i],v[j]])])\n",
    "            boundary[i, j] = (poly_map[0].dot(np.array(theta)))\n",
    "\n",
    "    return boundary\n",
    "\n",
    "#plot data and boundary\n",
    "fig = plt.figure()\n",
    "\n",
    "u = np.linspace(-1.1, 1.1, 50)\n",
    "v = np.linspace(-1.1, 1.1, 50)\n",
    "\n",
    "boundary = get_boundary(u, v, w_star, order)\n",
    "\n",
    "plt.title('microchips')\n",
    "plt.xlabel('test 1')\n",
    "plt.ylabel('test 2')\n",
    "plt.scatter(X1,X2, c=y.ravel())\n",
    "plt.contour(u, v, boundary, 0, colors='red')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the regularized logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.score(Xmap, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Tuning the hyper-parameters\n",
    "Try tuning the two hyper-parameters ($C$ and the polynome order) and see how the decision boundary and the model's accuracy evolve.\n",
    "\n",
    "### Use a grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = np.zeros((10, 20))\n",
    "\n",
    "C_range = list(10**x for x in range (0, 10))\n",
    "\n",
    "for idx, c in enumerate(C_range):\n",
    "    print(idx, sep='.', end='', flush=True) \n",
    "    for order in range(1,21):\n",
    "        poly = PolynomialFeatures(order)\n",
    "        Xmap = poly.fit_transform(X)\n",
    "        \n",
    "        clf = LogisticRegression(random_state=0, solver='lbfgs', multi_class='multinomial', C=c).fit(Xmap, y)\n",
    "        \n",
    "        acc[idx,order-1] = clf.score(Xmap, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get  $\\lambda^*$ and $order^*$ (those maximizing the accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import unravel_index\n",
    "acc_max_idx = unravel_index(acc.argmax(), acc.shape)\n",
    "print(acc_max_idx)\n",
    "print(acc[acc_max_idx[0], acc_max_idx[1]])\n",
    "\n",
    "\n",
    "c_star = C_range[acc_max_idx[0]]\n",
    "order_star = acc_max_idx[1]\n",
    "\n",
    "print(\"c_star = \", c_star, \", order_star = \", order_star)\n",
    "\n",
    "\n",
    "fig = plt.figure()\n",
    "fig.clf()\n",
    "ax = fig.add_subplot(1,1,1)\n",
    "img = ax.imshow(acc, interpolation='nearest', vmin=0.0, vmax=1.0)\n",
    "fig.colorbar(img)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### plot data and boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "\n",
    "u = np.linspace(-1.1, 1.1, 50)\n",
    "v = np.linspace(-1.1, 1.1, 50)\n",
    "\n",
    "poly = PolynomialFeatures(order_star)\n",
    "Xmap = poly.fit_transform(X)\n",
    "clf = LogisticRegression(random_state=0, solver='lbfgs', multi_class='multinomial', C=c_star).fit(Xmap, y)\n",
    "theta_star =  clf.coef_[0]     \n",
    "\n",
    "\n",
    "boundary_green = get_boundary(u, v, theta_star, order_star)\n",
    "\n",
    "plt.title('score=%f' %clf.score(Xmap, y))\n",
    "plt.xlabel('test 1')\n",
    "plt.ylabel('test 2')\n",
    "plt.scatter(X1,X2, c=y.ravel())\n",
    "plt.contour(u, v, boundary_green, 0, colors='green')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1) use pipelines\n",
    "#1) try GridSearch and Randomised Search\n",
    "#2) try SVM with different Kernels\n",
    "#3) try GridSearch and Randomised Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}